{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ee98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D,ZeroPadding2D\n",
    "from keras.layers import Input, Lambda\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Reshape, Activation, Multiply, Concatenate, Flatten, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fba0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24897b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# # tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "# tf.config.set_visible_devices([physical_devices[2]], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ecaac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define aspects of the model and create instances of both the \n",
    "# test and train batch generators and the complete model.\n",
    "\n",
    "imsize = 28\n",
    "batch_size = 32\n",
    "embedding_dim = 2 \n",
    "LR = 0.0001\n",
    "EPOCHS = 5\n",
    "alpha = 0.2 \n",
    "input_x=20\n",
    "input_y=125\n",
    "total_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d088c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_image_count(file_path):\n",
    "    # Open the file in binary mode for reading\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Load the data from the file using pickle\n",
    "        loaded_data = pickle.load(f)\n",
    "\n",
    "    # Check if the loaded data is a numpy array\n",
    "    if isinstance(loaded_data, np.ndarray):\n",
    "        # If loaded data is a numpy array, return the number of images (third dimension)\n",
    "        return loaded_data.shape[2]\n",
    "    else:\n",
    "        # If loaded data is not a numpy array, return 0 (indicating no images)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e98d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "global_path = \"\"\n",
    "model_folder_name = \"model_cbam\" \n",
    "\n",
    "# model_path = f'{global_path}/model_history/{model_folder_name}'\n",
    "model_path = f\"model_history/{model_folder_name}\"\n",
    "\n",
    "create_folder_if_not_exists(model_path)\n",
    "data_folder_path=\"data\"\n",
    "# history_path = os.path.join(global_path, f\"model_history/{checkpoint_folder_name}/history/\")\n",
    "# checkpoint_path = os.path.join(global_path, f\"model_history/{checkpoint_folder_name}/checkpoint/\")\n",
    "# Specify the directory path\n",
    "X_seiz_train=[]\n",
    "X_seiz_test=[]\n",
    "\n",
    "X_bckg_train=[]\n",
    "X_bckg_test=[]\n",
    "\n",
    "image_count=20000  # How much images to load for trainig or testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bee72e",
   "metadata": {},
   "source": [
    "## Save model and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model_path = f\"{model_path}/emd_{model_folder_name}.h5\"\n",
    "emd_history_log_path = f\"{model_path}/emd_{model_folder_name}.csv\"\n",
    "\n",
    "cls_model_path = f\"{model_path}/cls_{model_folder_name}.h5\"\n",
    "cls_history_log_path = f\"{model_path}/cls_{model_folder_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_image_arr(test=0):\n",
    "    global data_folder_path  # Assuming data_folder_path is defined globally\n",
    "\n",
    "    # Initialize lists to store image data\n",
    "    X_bckg_train, X_bckg_test, X_seiz_train, X_seiz_test = [], [], [], []\n",
    "\n",
    "    # print(\"   --reset image started at\", time.time(), test, len(X_bckg_train), len(X_bckg_test), len(X_seiz_train), len(X_seiz_test))\n",
    "\n",
    "    if test == 2:\n",
    "        # Define paths for seizure and background images in the test set\n",
    "        seiz_folder_path_test = os.path.join(data_folder_path, \"test/seiz\")\n",
    "        bckg_folder_path_test = os.path.join(data_folder_path, \"test/bckg\")\n",
    "        seiz_file_names_test = os.listdir(seiz_folder_path_test)\n",
    "        bckg_file_names_test = os.listdir(bckg_folder_path_test)\n",
    "\n",
    "        # Load seizure images for test set\n",
    "        while len(X_seiz_test) < image_count:\n",
    "            i = random.randint(0, len(seiz_file_names_test) - 1)\n",
    "            file_p = os.path.join(seiz_folder_path_test, seiz_file_names_test[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]     # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_seiz_test.append(loaded_data[:, :, i])\n",
    "\n",
    "        # Load background images for test set\n",
    "        while len(X_bckg_test) < image_count:\n",
    "            i = random.randint(0, len(bckg_file_names_test) - 1)\n",
    "            file_p = os.path.join(bckg_folder_path_test, bckg_file_names_test[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]    # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_bckg_test.append(loaded_data[:, :, i])\n",
    "\n",
    "    else:\n",
    "        # Define paths for seizure and background images in the train and validation sets\n",
    "        seiz_folder_path_train = os.path.join(data_folder_path, \"train/seiz\")\n",
    "        bckg_folder_path_train = os.path.join(data_folder_path, \"train/bckg\")\n",
    "        seiz_folder_path_val = os.path.join(data_folder_path, \"val/seiz\")\n",
    "        bckg_folder_path_val = os.path.join(data_folder_path, \"val/bckg\")\n",
    "\n",
    "        seiz_file_names_train = os.listdir(seiz_folder_path_train)\n",
    "        bckg_file_names_train = os.listdir(bckg_folder_path_train)\n",
    "        seiz_file_names_val = os.listdir(seiz_folder_path_val)\n",
    "        bckg_file_names_val = os.listdir(bckg_folder_path_val)\n",
    "\n",
    "        # Load seizure images for training set\n",
    "        while len(X_seiz_train) < image_count * 0.8:\n",
    "            i = random.randint(0, len(seiz_file_names_train) - 1)\n",
    "            file_p = os.path.join(seiz_folder_path_train, seiz_file_names_train[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]      # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_seiz_train.append(loaded_data[:, :, i])\n",
    "\n",
    "        # Load seizure images for validation set\n",
    "        while len(X_seiz_test) < image_count * 0.2:\n",
    "            i = random.randint(0, len(seiz_file_names_val) - 1)\n",
    "            file_p = os.path.join(seiz_folder_path_val, seiz_file_names_val[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]       # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_seiz_test.append(loaded_data[:, :, i])\n",
    "\n",
    "        # Load background images for training set\n",
    "        while len(X_bckg_train) < image_count * 0.8:\n",
    "            i = random.randint(0, len(bckg_file_names_train) - 1)\n",
    "            file_p = os.path.join(bckg_folder_path_train, bckg_file_names_train[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]       # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_bckg_train.append(loaded_data[:, :, i])\n",
    "\n",
    "        # Load background images for validation set\n",
    "        while len(X_bckg_test) < image_count * 0.2:\n",
    "            i = random.randint(0, len(bckg_file_names_val) - 1)\n",
    "            file_p = os.path.join(bckg_folder_path_val, bckg_file_names_val[i])\n",
    "            with open(file_p, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "\n",
    "            # Randomly select a fraction of images from the loaded data\n",
    "            n = loaded_data.shape[2]      # Number of images in current pkl file\n",
    "            x = min(1000, max(1, n // 4))\n",
    "            idx_arr = set()\n",
    "            while len(idx_arr) < x:\n",
    "                idx_arr.add(random.randint(0, n - 1))\n",
    "            for i in idx_arr:\n",
    "                X_bckg_test.append(loaded_data[:, :, i])\n",
    "\n",
    "    # print(\"   --reset image successfully at\", time.time(), test, len(X_bckg_train), len(X_bckg_test), len(X_seiz_train), len(X_seiz_test))\n",
    "    return X_seiz_train, X_seiz_test, X_bckg_train, X_bckg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd75387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(label, test=0):\n",
    "    \"\"\"\n",
    "    Choose an image from our training or test data with the given label.\n",
    "\n",
    "    Args:\n",
    "        label (int): Label indicating the type of image (0 for background, 1 for seizure).\n",
    "        test (int): Indicator for whether to select from test data (default 0 for training data).\n",
    "\n",
    "    Returns:\n",
    "        img: Selected image.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Check label to determine whether to select background or seizure image\n",
    "    if label == 0:  # Background image\n",
    "        if test != 0:  # If test set\n",
    "            idx = np.random.randint(len(X_bckg_test))  # Choose random index from test data\n",
    "            img = X_bckg_test[idx]  # Select image\n",
    "        else:  # If training set\n",
    "            idx = np.random.randint(len(X_bckg_train))  # Choose random index from training data\n",
    "            img = X_bckg_train[idx]  # Select image\n",
    "    else:  # Seizure image\n",
    "        if test != 0:  # If test set\n",
    "            idx = np.random.randint(len(X_seiz_test))  # Choose random index from test data\n",
    "            img = X_seiz_test[idx]  # Select image\n",
    "        else:  # If training set\n",
    "            idx = np.random.randint(len(X_seiz_train))  # Choose random index from training data\n",
    "            img = X_seiz_train[idx]  # Select image\n",
    "    \n",
    "    # nan_positions = np.argwhere(np.isnan(img))\n",
    "    # if len(nan_positions) > 0:\n",
    "    #     print(\"heheheheheheheheheh\")\n",
    "\n",
    "    return img  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b340bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(test=0):\n",
    "    \"\"\"\n",
    "    Generate an un-ending stream (i.e., a generator) of classification data for training or test.\n",
    "\n",
    "    Args:\n",
    "        test (int): Indicator for whether to generate data from test data (default 0 for training data).\n",
    "\n",
    "    Yields:\n",
    "        tuple: A tuple containing images and their corresponding labels.\n",
    "    \"\"\"\n",
    "    global X_seiz_train, X_seiz_test, X_bckg_train, X_bckg_test\n",
    "\n",
    "    # Infinite loop to continuously generate classification data\n",
    "    limit = 0  # Counter for resetting image arrays\n",
    "    while True:\n",
    "        if limit % 200 == 0:\n",
    "            # print(\"--Calling reset_image at \", time.time())  # Print timestamp when resetting images\n",
    "            X_seiz_train, X_seiz_test, X_bckg_train, X_bckg_test = reset_image_arr(test)  # Reset image arrays\n",
    "\n",
    "        a = []  # List to store images\n",
    "        label = []  # List to store labels\n",
    "\n",
    "        # Generate images and labels for each batch\n",
    "\n",
    "        for _ in range(batch_size-4):\n",
    "            img_class = np.random.randint(2)\n",
    "            img = get_image(img_class, test)  # Get a image with label img_class\n",
    "            a.append(img)\n",
    "            label.append(img_class)  # Add label 0 for background image or 1 for seizure image\n",
    "        \n",
    "        img_0 = get_image(0, test)  # Get a image with label 0\n",
    "        a.append(img_0)\n",
    "        label.append(0)\n",
    "\n",
    "        img_0 = get_image(0, test)  # Get a image with label 0\n",
    "        a.append(img_0)\n",
    "        label.append(0)\n",
    "\n",
    "        img_1 = get_image(1, test)  # Get a image with label 1\n",
    "        a.append(img_1)\n",
    "        label.append(1)\n",
    "\n",
    "        img_1 = get_image(1, test)  # Get a image with label 1\n",
    "        a.append(img_1)\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        A = np.array(a, dtype='float32')\n",
    "        label = np.array(label)\n",
    "\n",
    "        # Shuffle the arrays while maintaining the correspondence between image and label\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(A)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(label)\n",
    "        label = tf.cast(label, dtype=tf.int32)\n",
    "        # Yield the image data and labels\n",
    "        yield [A], label\n",
    "\n",
    "        # Increment limit counter\n",
    "        limit += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970164f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_loss(labels, embeddings):\n",
    "    ans = hard_triplet_loss(labels, embeddings, 0.2)\n",
    "    return ans \n",
    "\n",
    "def triplet_lossss(x, alpha = 0.2):\n",
    "    # Triplet Loss function.\n",
    "    anchor,positive,negative = x\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "    return loss\n",
    "\n",
    "def hard_triplet_loss(label, emd, margin=0.2):\n",
    "    # Compute pairwise distance matrix\n",
    "    pairwise_dist = tf.norm(tf.expand_dims(emd, axis=1) - tf.expand_dims(emd, axis=0), axis=-1)\n",
    "\n",
    "    # Get positive and negative mask\n",
    "    label_equal = tf.equal(tf.expand_dims(label, axis=1), tf.expand_dims(label, axis=0))\n",
    "    positive_mask = tf.cast(label_equal, dtype=tf.float32)\n",
    "    negative_mask = 1.0 - positive_mask\n",
    "\n",
    "    # Compute hardest positive and hardest negative distances\n",
    "    max_positive_dist = tf.reduce_max(pairwise_dist * positive_mask, axis=1)\n",
    "    min_negative_dist = tf.reduce_min((pairwise_dist + 1e6) * negative_mask, axis=1)\n",
    "\n",
    "    # Compute triplet loss\n",
    "    loss = tf.maximum(0.0, max_positive_dist - min_negative_dist + margin)\n",
    "\n",
    "    # Compute mean loss over the batch\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "def cbam_block(inputs, reduction_ratio=16):\n",
    "    # Channel Attention\n",
    "    channel = inputs.shape[-1]\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(inputs)\n",
    "    max_pool = GlobalMaxPooling2D()(inputs)\n",
    "    \n",
    "    avg_pool = Dense(channel // reduction_ratio, activation='relu')(avg_pool)\n",
    "    avg_pool = Dense(channel)(avg_pool)\n",
    "    \n",
    "    max_pool = Dense(channel // reduction_ratio, activation='relu')(max_pool)\n",
    "    max_pool = Dense(channel)(max_pool)\n",
    "    \n",
    "    channel_attention = Activation('sigmoid')(avg_pool + max_pool)\n",
    "    channel_attention = Multiply()([inputs, Reshape((1, 1, channel))(channel_attention)])\n",
    "    \n",
    "    # Spatial Attention\n",
    "    avg_pool = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
    "    \n",
    "    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "    spatial_attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)\n",
    "    \n",
    "    cbam_output = Multiply()([channel_attention, spatial_attention])\n",
    "    return cbam_output\n",
    "\n",
    "def embedding_model():\n",
    "    input_layer = Input(shape=(input_x, input_y, 1), name='input_layer')\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', name='convolution_1')(input_layer)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', name='convolution_2')(x)\n",
    "    \n",
    "    # Adding CBAM block after convolutional layers\n",
    "    x = cbam_block(x)\n",
    "    \n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='max_pooling')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(256, activation='relu', name='dense_lol_1')(x)\n",
    "    x = Dense(64, name='dense_lol_2')(x)\n",
    "    output_layer = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def complete_model(base_model):\n",
    "    input_1 = Input((input_x, input_y,1), name='input_layer_A')\n",
    "    A = base_model(input_1)\n",
    "\n",
    "    model = Model(inputs=[input_1], outputs=A)\n",
    "    model.compile(\n",
    "        # loss=tfa.losses.TripletSemiHardLoss(),\n",
    "                  loss = mean_loss,\n",
    "                  optimizer=Adam(LR))\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fd5f0",
   "metadata": {},
   "source": [
    "## Intialize data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e12e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator_emd = data_loader()\n",
    "val_generator_emd = data_loader(test=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_generator_emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7826e",
   "metadata": {},
   "source": [
    "## Initialize Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model = embedding_model()\n",
    "cmp_model = complete_model(emd_model)\n",
    "cmp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db490e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8180b4",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712324eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "# Custom callback to log loss and accuracy to CSV\n",
    "class CSVLogger(Callback):\n",
    "    def __init__(self, LOG_PATH):\n",
    "        self.log_path = LOG_PATH\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, log_path, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        with open(self.log_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            if epoch == 0:\n",
    "                # Write the header\n",
    "                writer.writerow(['epoch'] + list(logs.keys()))\n",
    "            # Write the epoch number and metrics\n",
    "            writer.writerow([epoch] + [logs[key] for key in logs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using triplet images provided by the train batch generator.\n",
    "# Save the trained weights.\n",
    "\n",
    "\n",
    "# Define a callback to save the model weights after each epoch\n",
    "checkpoint_callback_emd = ModelCheckpoint(filepath=emd_model_path, \n",
    "                                      save_weights_only=False,\n",
    "                                      verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "csv_logger_emd = CSVLogger(LOG_PATH=emd_history_log_path)\n",
    "\n",
    "# Fit the model with the callbacks\n",
    "history = cmp_model.fit_generator(train_generator_emd,\n",
    "                                  validation_data=val_generator_emd,\n",
    "                                  epochs=20,\n",
    "                                  verbose=1,\n",
    "                                  steps_per_epoch=1000,\n",
    "                                  validation_steps=30,\n",
    "                                  callbacks=[checkpoint_callback_emd, csv_logger_emd])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa1677",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d00b0f",
   "metadata": {},
   "source": [
    "## Initialize Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model_arc = embedding_model()\n",
    "complete_model = complete_model(emd_model_arc)\n",
    "complete_model.load_weights(emd_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11425fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f6869",
   "metadata": {},
   "source": [
    "## Load embedding model from complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4e70e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "emd_model = embedding_model()\n",
    "\n",
    "emd_model = complete_model.get_layer('model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_model(embedding_model):\n",
    "    \"\"\"\n",
    "    Create a classification model on top of the embedding model.\n",
    "    \n",
    "    Args:\n",
    "    embedding_model: Pretrained embedding model\n",
    "    \n",
    "    Returns:\n",
    "    classification_model: Model for classification on top of the embedding model\n",
    "    \"\"\"\n",
    "    # Freeze the layers of the embedding model\n",
    "    for layer in embedding_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_x, input_y, 1), name='input_layer'))  # Input layer\n",
    "    model.add(embedding_model)\n",
    "    # model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))  # 2 classes for binary classification\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a6ef5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the classification model\n",
    "classifier_model = classification_model(emd_model)\n",
    "\n",
    "# Compile the model\n",
    "classifier_model.compile(optimizer='adam',\n",
    "                          loss='sparse_categorical_crossentropy',\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7f774",
   "metadata": {},
   "source": [
    "## Initialize data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0ad36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_seiz_train, X_seiz_test, X_bckg_train, X_bckg_test=reset_image_arr()\n",
    "train_generator_cls=data_loader()\n",
    "val_generator_cls=data_loader(test=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9520d",
   "metadata": {},
   "source": [
    "## Training classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback_cls = ModelCheckpoint(filepath=cls_model_path, \n",
    "                                      save_weights_only=False,\n",
    "                                      verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "csv_logger_cls = CSVLogger(LOG_PATH=cls_history_log_path)\n",
    "\n",
    "# Fit the model with the callbacks\n",
    "history = classifier_model.fit_generator(train_generator_cls,\n",
    "                                  validation_data=val_generator_cls,\n",
    "                                  epochs=20,\n",
    "                                  verbose=1,\n",
    "                                  steps_per_epoch=5,\n",
    "                                  validation_steps=2,\n",
    "                                  callbacks=[checkpoint_callback_cls, csv_logger_cls])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2d202",
   "metadata": {},
   "source": [
    "## Load classification and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01700",
   "metadata": {},
   "outputs": [],
   "source": [
    "emd_model_arc = embedding_model()\n",
    "classifier_model_final = classification_model(emd_model_arc)\n",
    "classifier_model_final.load_weights(cls_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def test_complete_folder(pkl_folder_path, label,model):\n",
    "    file_name_arr = os.listdir(pkl_folder_path)\n",
    "    c_pkl = 0\n",
    "    c_img = 0\n",
    "    c_true_pkl = 0\n",
    "    c_true_img = 0\n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    d=[]\n",
    "    for i in range(len(file_name_arr)):\n",
    "        file_name = file_name_arr[i]\n",
    "        file_path = os.path.join(pkl_folder_path,file_name)\n",
    "        with open(file_path, 'rb') as f:\n",
    "            # Load the data from the file using pickle\n",
    "            loaded_data = pickle.load(f)\n",
    "        # print(\"-->\",len(file_name_arr),i,loaded_data.shape[2])\n",
    "        loaded_data=np.transpose(loaded_data, (2, 0, 1))\n",
    "        loaded_data=np.expand_dims(loaded_data, axis=-1)\n",
    "        # print(\"-->\",len(file_name_arr),i,loaded_data.shape)\n",
    "        y_pred = model.predict(loaded_data)\n",
    "\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        c_pkl+=1 \n",
    "        c_img+=len(y_pred)\n",
    "\n",
    "        curr_true = 0\n",
    "        for curr_label in y_pred:\n",
    "            if curr_label==label:\n",
    "                c_true_img+=1 \n",
    "                curr_true+=1 \n",
    "\n",
    "        a.append(len(y_pred))\n",
    "        b.append(curr_true)\n",
    "        \n",
    "        print(len(file_name_arr),i,\"-------->>>\",curr_true)\n",
    "        if curr_true>len(y_pred)//2:\n",
    "            c_true_pkl+=1 \n",
    "            c.append(1)\n",
    "        else:\n",
    "            c.append(0)\n",
    "        d.append(file_name)\n",
    "\n",
    "        # break \n",
    "    df = pd.DataFrame({\"file_name\":d, \"total_images\":a, \"true_predicted\":b, \"is_good_pkl\":c})\n",
    "    df.to_excel(f\"model_history/{model_folder_name}/results_{label}.xlsx\", index=False)\n",
    "    return [c_img, c_pkl, c_true_img, c_true_pkl]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7196871",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "num_img = 0\n",
    "num_pkl = 0\n",
    "\n",
    "correct_img = 0\n",
    "correct_pkl = 0\n",
    "\n",
    "bckg_path = \"data/val/bckg\"\n",
    "seiz_path = \"data/val/seiz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd376034",
   "metadata": {},
   "outputs": [],
   "source": [
    "bckg_results  = test_complete_folder(bckg_path,0,classifier_model_final)\n",
    "\n",
    "print(bckg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e2a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seiz_results  = test_complete_folder(seiz_path,1,classifier_model_final)\n",
    "\n",
    "print(seiz_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036b2f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy img\",(seiz_results[2]+bckg_results[2])/(seiz_results[0]+bckg_results[0]))\n",
    "print(\"Accuracy pkl\",(seiz_results[3]+bckg_results[3])/(seiz_results[1]+bckg_results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba948275",
   "metadata": {},
   "source": [
    "## Inferences for Image-level prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_img = [1] * seiz_results[0] + [0] * bckg_results[0]  # assuming n and m are the counts of class 1 and class 2 images respectively\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels_img = [1] * seiz_results[2] + [0] * (seiz_results[0]-seiz_results[2]) +[0] * bckg_results[2] + [1] * (bckg_results[0]-bckg_results[2]) # assuming a and b are the counts of correct predictions for class 1 and class 2 respectively\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_img, predicted_labels_img)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix(image)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and print classification report\n",
    "print(classification_report(true_labels_img, predicted_labels_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174e5ac",
   "metadata": {},
   "source": [
    "## Patient level prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf18fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_pkl = [1] * seiz_results[1] + [0] * bckg_results[1]  # assuming n and m are the counts of class 1 and class 2 images respectively\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels_pkl = [1] * seiz_results[3] + [0] * (seiz_results[1]-seiz_results[3]) +[0] * bckg_results[3] + [1] * (bckg_results[1]-bckg_results[3]) # assuming a and b are the counts of correct predictions for class 1 and class 2 respectively\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_pkl, predicted_labels_pkl)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix(pkl)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute and print classification report\n",
    "print(classification_report(true_labels_pkl, predicted_labels_pkl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c919f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
